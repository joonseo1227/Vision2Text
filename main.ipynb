{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "473becbfa6fb8cff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T10:25:05.011344Z",
     "start_time": "2024-12-08T10:25:04.750175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import cv2  # OpenCV library for image processing\n",
    "import torch  # PyTorch library for machine learning and deep learning tasks\n",
    "from PIL import Image  # Python Imaging Library (PIL) for handling and processing images\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration  # Hugging Face Transformers for BLIP model\n",
    "import matplotlib.pyplot as plt  # Matplotlib for image visualization\n",
    "import os  # OS module to handle file paths and system operations\n",
    "\n",
    "# Enable inline display of Matplotlib plots in Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Print confirmation message indicating that all libraries have been successfully imported\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55433c34e23b04bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T10:25:54.050283Z",
     "start_time": "2024-12-08T10:25:07.171875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# BLIP model initialization\n",
    "try:\n",
    "    # Load the processor and model for BLIP (Bootstrapped Language-Image Pretraining)\n",
    "    # \"Salesforce/blip-image-captioning-base\" is a pre-trained model for image captioning\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "    # Determine the computation device: use GPU if available, otherwise fallback to CPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Move the model to the selected device (GPU or CPU)\n",
    "    model.to(device)\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any errors that occur during model initialization\n",
    "    print(f\"An error occurred during model initialization: {str(e)}\")\n",
    "    raise  # Re-raise the exception to halt execution if initialization fails\n",
    "\n",
    "else:\n",
    "    # Confirm successful model loading and display the active device\n",
    "    print(f\"Model loaded successfully. Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bad928de26d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T10:26:37.006682Z",
     "start_time": "2024-12-08T10:26:37.001693Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using the BLIP (Bootstrapped Language-Image Pretraining) model.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path (str): The file path of the input image.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated caption for the image, or an error message if an issue occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the image file exists at the given path\n",
    "        if not os.path.exists(image_path):\n",
    "            return \"The image file does not exist.\"\n",
    "\n",
    "        # Read the image from the file path using OpenCV\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return \"The image cannot be read. It might be corrupted or not a valid image file.\"\n",
    "\n",
    "        # Convert the image from BGR (OpenCV's default format) to RGB (used by PIL and models)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Convert the image (NumPy array) to a PIL Image for compatibility with the model processor\n",
    "        pil_image = Image.fromarray(img_rgb)\n",
    "\n",
    "        # Process the image and generate a caption using the BLIP model\n",
    "        # The processor prepares the image as input for the model\n",
    "        inputs = processor(pil_image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate a caption for the image with a specified maximum length\n",
    "        output = model.generate(**inputs, max_length=50)\n",
    "        \n",
    "        # Decode the output tokens into a human-readable caption\n",
    "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        try:\n",
    "            # Visualize the image with the generated caption\n",
    "            plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "            plt.imshow(img_rgb)  # Display the RGB image\n",
    "            plt.axis('off')  # Remove axis for better visualization\n",
    "            plt.title(caption)  # Set the title to the generated caption\n",
    "            plt.show()\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during visualization\n",
    "            print(f\"Error occurred while visualizing the result: {str(e)}\")\n",
    "            return caption  # Return the caption even if visualization fails\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any other errors that occur during the process\n",
    "        return f\"An error occurred while generating the caption: {str(e)}\"\n",
    "\n",
    "    # Return the generated caption as the final result\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef63e23b3728f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T10:26:41.243427Z",
     "start_time": "2024-12-08T10:26:38.661739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Caption: An error occurred while generating the caption: name 'os' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Test the generate_caption function with a sample image\n",
    "\n",
    "# Path to the test image\n",
    "image_path = \"./images/image.jpg\"\n",
    "\n",
    "# Call the generate_caption function to generate a caption for the image\n",
    "# The function handles image loading, processing, caption generation, and visualization\n",
    "caption = generate_caption(image_path)\n",
    "\n",
    "# Print the generated caption to the console\n",
    "print(f\"\\nGenerated Caption: {caption}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
